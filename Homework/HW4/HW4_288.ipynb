{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "\n",
    "## <em> Fisher Information Matrix, Independent Component Analysis</em>\n",
    "<br>\n",
    "This notebook is arranged in cells. Texts are usually written in the markdown cells, and here you can use html tags (make it bold, italic, colored, etc). You can double click on this cell to see the formatting.<br>\n",
    "<br>\n",
    "The ellipsis (...) are provided where you are expected to write your solution but feel free to change the template (not over much) in case this style is not to your taste. <br>\n",
    "<br>\n",
    "<em>Hit \"Shift-Enter\" on a code cell to evaluate it.  Double click a Markdown cell to edit. </em><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> NOTE: Apologies, we are experiencing some issues with OK (no file uploaded by running `ok.submit()`). Few other classes are having similar issues, and currently we do not have a solution. </span>\n",
    "#### <span style=\"color:red\"> Hence, please download your Jupyter notebook as an .ipynb file and upload it as a homework submission in bcourses. A detailed instruction is available here (scroll down to \"Instruction\"):https://phy188-288-ucb.github.io/seljak-fall-2020/homeworks/.  </span>\n",
    "#### <span style=\"color:red\"> Before you save your work, please make sure that you run all cells and show all outputs.  </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Constraining the cosmological parameters using the Planck power spectrum\n",
    "\n",
    "<i>Planck</i> is the third-generation space telescope, following COBE and WMAP, and it aims to determine the geometry and content of the Universe by observing the cosmic microwave background radiation (CMB), emitted around 380,000 years after the Big Bang. Permeating the whole universe and containing information on the properties of the early Universe, the CMB is widely known as the strongest evidence for the Big Bang model. <br><br>\n",
    "Measuring the spectrum of the CMB, we confirm that it is very close to the radiation from an ideal blackbody, and flunctuations in the spectrum are very small. Averaging ocer all locations, its mean temperature is $2.725K$, and its root mean square temperature fluctuation is $\\langle(\\frac{\\delta T}{T})^2\\rangle^{1/2} = 1.1 \\times 10^{-5}$ (i.e. the temperature of the CMB varies by only ~ 30 $\\mu K$ across the sky). <br>\n",
    "![alt text](Planck.png \"Title\")\n",
    "<br>\n",
    "Suppose you observe the fluctuations $\\delta T/T$. Since we are taking measurements on the surface of a sphere, it is useful to expand $\\delta T/T$ in spherical harmonics (because they form a complete set of orthogonal functions on the sphere):<br>\n",
    "$$ \\frac{\\delta T}{T} (\\theta, \\phi) = \\sum_{l = 0}^{\\infty} \\sum_{m = -l}^{l} \\mathrm{a}_{lm} \\mathrm{Y}_{lm} (\\theta, \\phi) $$\n",
    "<br>\n",
    "In flat space, we can do a Fourier transform of a function $f(x)$ as $\\sum_k \\mathrm{a}_k \\mathrm{e}^{ikx}$ where $k$ is the wavenumber, and $|\\mathrm{a}_k|$ determines the amplitude of the mode. For spherical harmonics, instead of $k$, we have $l$, the number of the modes along a meridian, and $m$, the number of modes along the equator. So $l$ and $m$ determine the wavelength ($\\lambda = 2\\pi/l$) and shape of the mode, respectively.\n",
    "<br><br>\n",
    "In cosmology, we are mostly interested in learning the statistical properties of this map and how different physical effects influence different physical scales, so it is useful to define the correlation function $C(\\theta)$ and split the CMB map into different scales. \n",
    "<br><br>\n",
    "Suppose that we observe $\\delta T/T$ at two different points on the sky. Relative to an observer, they are in direction $\\hat{n}$ and $\\hat{n}'$ and are separated by an angle $\\theta$ given by $cos\\theta = \\hat{n} \\cdot \\hat{n}'$ Then, we can find the correlation function by multiplying together the values of $\\delta T/T$ at the two points and average the product over all points separated by the angle $\\theta$.\n",
    "$$ C(\\theta)^{TT} =  \\Big\\langle \\frac{\\delta T}{T}(\\hat{n})\\frac{\\delta T}{T}(\\hat{n}') \\Big\\rangle_{\\hat{n} \\cdot \\hat{n}' = cos\\theta}$$\n",
    "<br><br>\n",
    "The above expression is specific to the temperature fluctuations, but we can also do a similar analysis for the polarization map of the CMB. (The CMB is polarized because it was scattered off of free electrons during decoupling.) We decompose the polarization pattern in the sky into a curl-free \"E-mode\" and grad-free \"B-mode.\" \n",
    "<br><br>\n",
    "However, the CMB measurements (limited by the experiment resolution and the patch of sky examined) tell us about $C(\\theta)$ over only a limited range of angular scales. (i.e. the precise values of $C(\\theta)$ for all angles from $\\theta = 0$ to $\\theta = 180^\\circ$ is not known.) Hence, using the expansion of $\\delta T/T$ in spherical harmonics, we write the correlation function as:\n",
    "$$ C(\\theta) = \\frac{1}{4\\pi}\\sum_{l=0}^\\infty (2l+1) C_l P_l(cos\\theta) $$\n",
    "where $P_l$ are the Legendre polynomials.\n",
    "<br><br>\n",
    "So we break down the correlation function into its multipole moments $C_l$, which is the angular power spectrum of the CMB.\n",
    "![alt text](multipoles.png \"Title\")\n",
    "<br><br>\n",
    "Remember that $\\lambda = 2\\pi/l$. So $C_l$ measures the amplitude as a function of wavelength. ($C_l = \\frac{1}{2l+1}\\sum_{m = -l}^l |\\mathrm{a}_{lm}|^2$). In this problem, we will consider the temperature power spectrum $C_l^{TT}$, the E-mode power spectrum $C_l^{EE} = \\frac{1}{2l+1}\\sum_{m = -l}^l |\\mathrm{a}_{lm}^E|^2$, and the temperature-polarization cross-correlation $C_l^{TE} = \\frac{1}{2l+1}\\sum_{m = -l}^l \\mathrm{a}_{lm}^{T*} \\mathrm{a}_{lm}^E$. \n",
    "<br><br>\n",
    "THe CMB angular power spectrum is usually expressed in terms of $D_l = l(l+1)C_l/2\\pi$ (in unit of $\\mu K^2$) because this better shows the contribution toward the variance of the temperature fluctuations.\n",
    "<br><br>\n",
    "Cosmologists built a software called \"cosmological boltzmann code\" which computes the theoretical power spectrum given cosmological parameters, such as the Hubble constant and the baryon density. Therefore, we can fit the theory power spectrum to the measured one in order to obtain the best-fit parameters.\n",
    "<br><br>\n",
    "Here, we consider six selected cosmological parameters, $\\vec{\\theta} = [\\theta_1, \\theta_2, ..., \\theta_6] = [H_0, \\Omega_b h^2, \\Omega_c h^2, n_s, A_s, \\tau]$. ($H_0$ = Hubble constant, $\\Omega_b h^2$ = physical baryon density parameter, $\\Omega_c h^2$ = physical cold dark matter density parameter, $n_s$ = scalar spectral index, $A_s$ = curvature fluctuation amplitude, $\\tau$ = reionization optical depth.). We provide you with the measured CMB power spectra from Planck Data Release 2.\n",
    "<br><br>\n",
    "<i>References</i> :\n",
    "<br>\n",
    "http://carina.fcaglp.unlp.edu.ar/extragalactica/Bibliografia/Ryden_IntroCosmo.pdf (Chapter 9, Intro to Cosmology, Barbara Ryden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fisher prediction for future CMB surveys\n",
    "\n",
    "In class, we learned that the Fisher information matrix is useful for designing an experiment; we can vary the experiment design and predict the level of the expected error on any given parameter. In this problem, we aim to determine how well a low-noise, high-resolution future CMB survey would do in constraining the cosmological parameters.\n",
    "<br><br>\n",
    "The Fisher matrix is defined as the ensemble average of the Hessian of the log-likelihood ($\\ln\\mathcal{L}$) with respect to the given parameters $\\vec{\\theta}$:\n",
    "<br><br>\n",
    "$$ F_{ij} = -\\left\\langle\\frac{\\partial^{2}\\ln\\mathcal{L}}{\\partial\\theta_i\\ \\partial\\theta_j}\\right\\rangle $$\n",
    "<br><br>\n",
    "Here we take the model CMB power spectrum as our observables. (Here we consider the auto-correlations $D_l^{TT}, D_l^{EE}$ and cross-correlation $D_l^{TE}$ obtained from the boltzmann code using the best-fit cosmological parameters from Planck, https://arxiv.org/pdf/1502.01589v3.pdf.) Then, we can estimate the Fisher matrix between two parameters $\\theta_i$ and $\\theta_j$ as:\n",
    "<br><br>\n",
    "$$ F_{ij} = \\sum_{l} \\sum_{k}\\frac{1}{(\\sigma_l^k)^2}\\frac{\\partial D^{k}_{\\ell}}{\\partial\\theta_i}\\frac{\\partial D^{k}_{\\ell}}{\\partial\\theta_j} $$\n",
    "<br><br>\n",
    "where we sum over the CMB auto- and cross-power spectra $D_l^k = [D_l^{0}, D_l^{1}, D_l^{2}] = [D_l^{TT}, D_l^{EE}, D_l^{TE}]$, and we assume that there is no correlation between them. $\\sigma^2$ is the variance of $D_l$ and noise $N_l$:\n",
    "<br><br>\n",
    "$$ (\\sigma_l^k)^2 = \\frac{2}{(2l+1) \\cdot f_{sky} \\cdot \\Delta l}(D_l^k + N_l^k)^2 $$\n",
    "<br><br>\n",
    "$f_{sky}$ is the fraction of the sky covered by the survey. Assume that $f_{sky} = 1$ for the sake of simplicity. $\\Delta l$ is the size of $l$-bin. Here, we set $l_{min} = 2, l_{max} = 2000$, and we have 92 $l$-bins in this range (For $2 \\leq l < 30$, the power spectra are not binned ($\\Delta l = 1$), and for $30 \\leq l < 2000$, they are binned, and the bin size is $\\Delta l = 30$). We obtain the measured and model power spectrum in that 92 $l$-bins.\n",
    "<br><br>\n",
    "In Part 1 and 2, the following is given:\n",
    "<br><br>\n",
    "(1) model power spectra ($D_l^{TT}, D_l^{EE}, D_l^{TE}$) evaluated at the best-fit values from Planck (i.e. assuming $\\vec{\\theta}_{best-fit}$)\n",
    "<br>\n",
    "(2) ($\\frac{\\partial D_l^{TT}}{\\partial H_0}\\Big\\vert_{\\vec{\\theta} = \\vec{\\theta}_{best-fit}}$, $\\frac{\\partial D_l^{TT}}{\\partial \\Omega_bh^2}\\Big\\vert_{\\vec{\\theta} = \\vec{\\theta}_{best-fit}}$, etc): its derivative with respect to the parameter $\\theta$ evaluated at the best-fit values from Planck (i.e. assuming $\\vec{\\theta}_{best-fit}$). These are the measurement errors on $D_l^{TT}, D_l^{EE}, D_l^{TE}$.\n",
    "<br>\n",
    "(3) the measurement error $\\sigma_{l, Planck}^{TT}, \\sigma_{l, Planck}^{EE}, \\sigma_{l, Planck}^{TE}$ assuming the noise from the current Planck survey\n",
    "<br>\n",
    "(4) (effective) $l$ values for which the spectra are measured. (i.e. these are the effective bin center values for each $l$ bin.)\n",
    "\n",
    "In Part 3 and 4, you assume a zero-noise futuristic survey, so you need to compute new measurement error $\\sigma_l$ assuming $N_l = 0$.\n",
    "<br><br>\n",
    "Finally, we can compute the Fisher matrix $F$ and obtain the covariance matrix $C$ by inverting $F$:\n",
    "$$ [C] = [F]^{-1} $$\n",
    "<br><br>\n",
    "<i>References:</i><br>\n",
    "Fisher Matrix Forecasting Review, Nicholas Kern <br>\n",
    "https://arxiv.org/pdf/0906.4123.pdf\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 1. First, load the measurement errors ($\\sigma_l^{TT}, \\sigma_l^{EE}, \\sigma_l^{TE}$), model power spectrum ($D_l^{TT}, D_l^{EE}, D_l^{TE}$) and their derivatives with respect to six cosmological parameters evaluated at the best-fit values from Planck ($\\frac{\\partial D_l^{TT}}{\\partial H_0}\\Big\\vert_{\\vec{\\theta} = \\vec{\\theta}_{best-fit}}$, $\\frac{\\partial D_l^{TT}}{\\partial \\Omega_bh^2}\\Big\\vert_{\\vec{\\theta} = \\vec{\\theta}_{best-fit}}$, etc). With the measurement errors from Planck, construct the Fisher matrix and the covariance matrix (you can use the numpy.linalg.inv for the matrix inversion). Evaluate the constraints on six parameters $\\sigma(H_0), \\sigma(\\Omega_bh^2), ... , \\sigma(\\tau)$ (corresponding to the square root of the diagonal entries of the covariance matrix). Print the results. </i></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Best-fit values of the cosmological parameters from https://arxiv.org/pdf/1502.01589v3.pdf\n",
    "H0     = 67.27\n",
    "ombh2  = 0.02225\n",
    "omch2  = 0.1198\n",
    "ns     = 0.9645\n",
    "As     = 2.2065e-9\n",
    "tau    = 0.079\n",
    "\n",
    "theta_best_Planck = np.array([H0, ombh2, omch2, ns, As, tau])\n",
    "\n",
    "data = np.loadtxt(\"Project1_EE_measured.dat\")\n",
    "# l (same for all power spectrum)\n",
    "l = data[:,0]\n",
    "\n",
    "# Planck noise\n",
    "\n",
    "# sigma_l for D_l^EE assuming Planck N_l\n",
    "data = np.loadtxt(\"Project1_EE_measured.dat\")\n",
    "sigma_l_Planck_EE = data[:,2]\n",
    "\n",
    "# sigma_l for D_l^TT assuming Planck N_l\n",
    "data = np.loadtxt(\"Project1_TT_measured.dat\")\n",
    "sigma_l_Planck_TT = data[:,2]\n",
    "\n",
    "# sigma_l for D_l^TE assuming Planck N_l\n",
    "data = np.loadtxt(\"Project1_TE_measured.dat\")\n",
    "sigma_l_Planck_TE = data[:,2]\n",
    "\n",
    "\n",
    "# Model power spectra given theta_best_Planck (calculated at the same l bins as the measured power spectrum)\n",
    "\n",
    "# D_l^EE (model)\n",
    "data = np.loadtxt(\"Project1_EE_model_at_theta_best_Planck.dat\")\n",
    "EE_model_Planck = data[:,1]\n",
    "\n",
    "# D_l^TT (model)\n",
    "data = np.loadtxt(\"Project1_TT_model_at_theta_best_Planck.dat\")\n",
    "TT_model_Planck = data[:,1]\n",
    "\n",
    "# D_l^TE (model)\n",
    "data = np.loadtxt(\"Project1_TE_model_at_theta_best_Planck.dat\")\n",
    "TE_model_Planck = data[:,1]\n",
    "\n",
    "\n",
    "# Derivative of the power spectrum given theta_best_Planck (calculated at the same l bins as the measured power spectrum)\n",
    "\n",
    "# Derivative of D_l^EE with respect to six parameters \n",
    "# ([theta1, theta2, theta3, theta4, theta5, theta6] = [H_0, \\Omega_b h^2, \\Omega_c h^2, n_s, A_s, \\tau])\n",
    "data = np.loadtxt(\"Project1_derivative_EE_at_theta_best_Planck.dat\")\n",
    "deriv_DlEE_theta1 = data[:,1]\n",
    "deriv_DlEE_theta2 = data[:,2]\n",
    "deriv_DlEE_theta3 = data[:,3]\n",
    "deriv_DlEE_theta4 = data[:,4]\n",
    "deriv_DlEE_theta5 = data[:,5]\n",
    "deriv_DlEE_theta6 = data[:,6]\n",
    "\n",
    "# Derivative of D_l^TT with respect to six parameters \n",
    "data = np.loadtxt(\"Project1_derivative_TT_at_theta_best_Planck.dat\")\n",
    "deriv_DlTT_theta1 = data[:,1]\n",
    "deriv_DlTT_theta2 = data[:,2]\n",
    "deriv_DlTT_theta3 = data[:,3]\n",
    "deriv_DlTT_theta4 = data[:,4]\n",
    "deriv_DlTT_theta5 = data[:,5]\n",
    "deriv_DlTT_theta6 = data[:,6]\n",
    "\n",
    "# Derivative of D_l^TE with respect to six parameters \n",
    "data = np.loadtxt(\"Project1_derivative_TE_at_theta_best_Planck.dat\")\n",
    "deriv_DlTE_theta1 = data[:,1]\n",
    "deriv_DlTE_theta2 = data[:,2]\n",
    "deriv_DlTE_theta3 = data[:,3]\n",
    "deriv_DlTE_theta4 = data[:,4]\n",
    "deriv_DlTE_theta5 = data[:,5]\n",
    "deriv_DlTE_theta6 = data[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from the covariance matrix, we can plot 1-d and 2-d constraints on the parameters. (See Fig. 6 in Planck 2015 paper https://arxiv.org/pdf/1502.01589v3.pdf)\n",
    "<br><br>\n",
    "<b>1-d constraint</b> (corresponding to the plots along the diagonal in Fig. 6, Planck 2015 paper): <br><br>\n",
    "First, the $i$th diagonal element of the covariance matrix correspond to $\\sigma({\\theta_i})^2$. Then, we can plot 1-d constraints on the parameter $\\theta_i$ assuming a normal distribution with mean = $(\\vec{\\theta}_{best-fit})_i$ and variance = $\\sigma({\\theta_i})^2$.\n",
    "<br><br>\n",
    "<b>2-d constraint</b> (off-diagonal plots in Fig. 6, Planck 2015 paper): <br><br>\n",
    "Consider two parameters $\\theta_i$ and $\\theta_j$ from $\\vec{\\theta}$. Now marginalize over other parameters - in order to marginalize over other parameters, you can simply remove those parameters' row and column from the full covariance matrix. (i.e. From the full covariance matrix, you know the variance of all six parameters and their covariances with each other. So build a smaller dimension - 2 x 2 - covariance matrix from this.) - and obtain a $2\\times2$ covariance matrix:\n",
    "<br><br>\n",
    "$$ \\mathrm{C_{ij}} =  \\binom{\\sigma({\\theta_i})^2\\ \\ \\ \\ \\ \\ \\mathrm{Cov}({\\theta_i, \\theta_j})}{\\mathrm{Cov}({\\theta_i, \\theta_j}) \\ \\ \\ \\ \\ \\ \\sigma({\\theta_j})^2} $$\n",
    "<br>\n",
    "Now, we can plot the 2-dimensional confidence region ellipses from this matrix. The lengths of the ellipse axes are the square root of the eigenvalues of the covariance matrix, and we can calculate the counter-clockwise rotation of the ellipse with the rotation angle:\n",
    "<br><br>\n",
    "$$ \\phi = \\frac{1}{2} \\mathrm{arctan}\\Big( \\frac{2\\cdot \\mathrm{Cov}(\\theta_i, \\theta_j)}{\\sigma({\\theta_i})^2-\\sigma({\\theta_j})^2} \\Big) = \\mathrm{arctan}(\\frac{\\vec{v_1}(y)}{\\vec{v_1}(x)}) $$\n",
    "<br>\n",
    "where $\\vec{v_1}$ is the eigenvector with the largest eigenvalue. So we calculate the angle of the largest eigenvector towards the x-axis to obtain the orientation of the ellipse. <br><br> \n",
    "Then, we multiply the axis lengths by some factor depending on the confidence level we are interested in. For 68%, this scale factor is $\\sqrt{\\Delta \\chi^2} \\approx 1.52$. For 95%, it is $\\sqrt{\\Delta \\chi^2} \\approx 2.48$.\n",
    "<br><br>\n",
    "Hint: For plotting ellipses, see HW3 Problem 1 Part 7.\n",
    "\n",
    "<span style=\"color:blue\"> <i> 2. From the covariance matrix, plot 1-d and 2-d constraints on the parameters. Note that the best-fit values of six parameters are alrady given in Part 1 (we just use the values from the Planck paper). For 2-d plot, show 68% and 95% confidence ellipses for pairs of parameters. Arrange those subplots in a triangle shape, as in Fig. 6, Planck 2015 (https://arxiv.org/pdf/1502.01589v3.pdf). </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assume that we have an ideal, zero-noise CMB survey with $N_l = 0$. However, we are still instrinsically limited on the number of independent modes we can measure (there are only (2l+1) of them) - $C_l = \\frac{1}{2l+1}\\sum_{m=-l}^{l}\\langle|a_{lm}|^2\\rangle$. This leads that we get an instrinsic error (called \"cosmic variance\") in our estimate of $C_l$. So we approximate that <br><br> $$ (\\sigma_l^{EE})^2 = \\frac{2}{(2l+1) \\cdot f_{sky} \\cdot \\Delta l}(D_l^{EE})^2,\\ \\ (\\sigma_l^{TT})^2 = \\frac{2}{(2l+1) \\cdot f_{sky} \\cdot \\Delta l}(D_l^{TT})^2,$$ <br> $$ (\\sigma_l^{TE})^2 = \\frac{2}{(2l+1) \\cdot f_{sky} \\cdot \\Delta l}\\frac{(D_l^{TE})^2 + D_l^{TT}D_l^{EE}}{2} $$.\n",
    "<br><br>\n",
    "\n",
    "<span style=\"color:blue\"> <i> 3. First compute $\\sigma_l$ for this zero-noise futuristic survey. (assuming $N_l^k = 0$) Repeat Part 1 and 2. (How well does a zero-noise CMB survey constrain the cosmologial parameters?) </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\"><i> 4. Combine Part 2 and Part 3 and compare. (First plot your results from Part 2 (1-d and 2-d constraints using the Planck power spectra and noise. Then, plot Part 3 results (assuming zero noise) on top with different colors. Note that your 1-d constrains in Part 3 are more sharply peaked Gaussians (with much smaller variances), so you can scale them so that its peak amplitudes match with your results from Part 2.) </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\"><i> 5. Starting from the best-fit values from the Planck 2015 paper, you constrained six cosmological parameters assuming that you have a zero-noise future CMB survey. Compare your results with Table 3 and Figure 6 in https://arxiv.org/pdf/1502.01589v3.pdf. </i></span><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:blue\"> <i> Answer: </i></span><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - Solving the cocktail party problem with ICA\n",
    "\n",
    "\"Independent component analysis was originally developed to deal with problems that are closely related to the cocktail-party problem. The goal is to find a linear representation of nongaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation.\" More details on ICA can be found in https://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we take the mixed sounds and images, and apply ICA tn them to separate the sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 1. Read the 3 sound files, and plot them as a function of time.  (In order to better see the features, you may plot them with some offsets.) </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs, data1 = wavfile.read('mix_sound1.wav')\n",
    "fs, data2 = wavfile.read('mix_sound2.wav')\n",
    "fs, data3 = wavfile.read('mix_sound3.wav')\n",
    "#fs is the sample rate, i.e., how many data points in one second.\n",
    "\n",
    "data = np.float64(np.c_[data1, data2, data3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "...\n",
    "\n",
    "plt.xlabel('Time [second]')\n",
    "plt.ylabel('Sound signal + offset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Now run the following cells and play the sounds. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio('mix_sound1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio('mix_sound2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio('mix_sound3.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell there are 3 signals mixed in these sounds. You can consider these sounds as recorded by 3 different recorders that have different distrance to the 3 sources. In orther words,\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    " \\bf X = A S + \\mu \\tag{1}\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "where $\\boldsymbol X$ is a vector of these 3 sounds, $\\boldsymbol S$ is a vector of 3 signals, $\\boldsymbol \\mu$ is a vector of the mean of $\\boldsymbol X$, and $\\boldsymbol A$ is the mixing matrix. \n",
    "<br><br>\n",
    "Next, using the sklearn's fastICA module, we will separate the signals from these sounds.\n",
    "<br><br>\n",
    "(1) Define the fastICA model:\n",
    "\n",
    "&nbsp; **ica = FastICA(algorithm='parallel')**\n",
    "\n",
    "(2) Using \"fit_transform,\" fit the model with the data and obtain the signals\n",
    "\n",
    "&nbsp; **S = ica.fit_transform(data)**\n",
    "\n",
    "<span style=\"color:blue\"> <i> 3. Find the 3 signals in the sound files. Plot the signals. (Again, you may plot them with some offsets.)</i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find the amplitude of the signals is very small. This is because fastICA whitens the data first before applying ICA, so that the covariance matrix of the signals is I. We can amplify the signals by multiplying them with a large number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. Now let's save the signals as wav files and play the sounds. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Amp = 1e6\n",
    "\n",
    "wavfile.write('signal_sound1.wav', fs, np.int16(Amp*S[:,0]))\n",
    "wavfile.write('signal_sound2.wav', fs, np.int16(Amp*S[:,1]))\n",
    "wavfile.write('signal_sound3.wav', fs, np.int16(Amp*S[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reconstruct the mixed sounds with the signals. The mixing matrix is given by ica.mixing_, and the mean of the data is given by ica.mean_. Note that the $X$ and $S$ from equation (1) are matrices of shape (Nsignal, Nsample), but the data and the signal you get from FastICA are matrices of shape (Nsample, Nsignal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 5. Reconstruct the sounds from the source signal, and show that the reconstruct sounds is very close to the given sounds using np.allclose</i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (np.allclose(X, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 6. The ICA requires the data to be centered and whitened. The FastICA module from sklearn does the data centering and whitening automatically. Now let's disable the data preprocessing in FastICA by ica = FastICA(whiten=False), and then redo the analysis in part 3 and 4. Plot and play the sounds. Does ICA work without data preprocessing?</i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 7. The principal companent analysis (PCA) also tries to interpret the underlying structure of the data by decomposing the data into linear combinations of the principal components. Now let's apply PCA to the sounds and see if the signals are cleanly separated in the principal components. Plot the principal components, save them as wav files and play the sounds. How does it compares to Part 3 and 4? </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at another example. Suppose now we have some linearly mixed images, and we are going to find the original photos with ICA. (This example is from https://github.com/vishwajeet97/Cocktail-Party-Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 8. Load in photos, and plot them. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img1=mpimg.imread('unos.jpg')\n",
    "img2=mpimg.imread('dos.jpg')\n",
    "img3=mpimg.imread('tres.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imag is a 2D array. You will need to flatten the data for the following analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 9. Redo the analysis in part 3 and 4. Separate the original photos and plot them. Note that the sign of the signals recovered by ICA may not be correct, so you probably need to multiply the photos by -1. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICA algorithm tries to find the most non-Gaussian directions of given data. FastICA uses the KL divergence between the data and standard Gaussian (negentropy) to charactrize the non-Gaussianity. Another way to measure non-Gaussianity is to use the Wasserstein distance between the data and standard Gaussian. In 1D, the Wasserstein distance, also called earth mover's distance, has a closed form solution using Cumulative Distribution Function (CDF). Below we provide you the code for doing ICA using Wasserstein distance. The code searches for the most non-Gaussian directions by maximizing the Wasserstein distance between the data and Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 10. Perform ICA on the mixed photos from Q9 (do the following steps) </i></span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Whitens the data with sklearn.decomposition.PCA(whiten=True)\n",
    "2. Run ICA with Wasserstein distance: $A$ = ICA_Wasserstein($X_{\\mathrm{whiten}}$)\n",
    "3. Recover the Signal S from the whitened data and mixing matrix $A$. Note that $\\mu=0$ because of the whitening, and the shape of $X$ of equation (1) is (Nsignal, Nsample).\n",
    "4. Plot the signals (original photos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def Percentile(input, percentiles):\n",
    "    \"\"\"\n",
    "    Find the percentiles of a tensor along the last dimension.\n",
    "    Adapted from https://github.com/aliutkus/torchpercentile/blob/master/torchpercentile/percentile.py\n",
    "    \"\"\"\n",
    "    percentiles = percentiles.double()\n",
    "    in_sorted, in_argsort = torch.sort(input, dim=-1)\n",
    "    positions = percentiles * (input.shape[-1]-1) / 100\n",
    "    floored = torch.floor(positions)\n",
    "    ceiled = floored + 1\n",
    "    ceiled[ceiled > input.shape[-1] - 1] = input.shape[-1] - 1\n",
    "    weight_ceiled = positions-floored\n",
    "    weight_floored = 1.0 - weight_ceiled\n",
    "    d0 = in_sorted[..., floored.long()] * weight_floored\n",
    "    d1 = in_sorted[..., ceiled.long()] * weight_ceiled\n",
    "    result = d0+d1\n",
    "    return result\n",
    "\n",
    "\n",
    "def SlicedWassersteinDistanceG(x, pg, q, p, perdim=True):\n",
    "    if q is None:\n",
    "        px = torch.sort(x, dim=-1)[0]\n",
    "    else:\n",
    "        px = Percentile(x, q)\n",
    "\n",
    "    if perdim:\n",
    "        WD = torch.mean(torch.abs(px-pg) ** p)\n",
    "    else:\n",
    "        WD = torch.mean(torch.abs(px-pg) ** p, dim=-1)\n",
    "    return WD\n",
    "\n",
    "\n",
    "def SWD_prepare(Npercentile=100, device=torch.device(\"cuda:0\"), gaussian=True):\n",
    "    start = 50 / Npercentile\n",
    "    end = 100-start\n",
    "    q = torch.linspace(start, end, Npercentile, device=device)\n",
    "    if gaussian:\n",
    "        pg = 2**0.5 * torch.erfinv(2*q/100-1)\n",
    "        return q, pg\n",
    "\n",
    "    \n",
    "def maxSWDdirection(x, x2='gaussian', n_component=None, maxiter=200, Npercentile=None, p=2, eps=1e-6):\n",
    "\n",
    "    #if x2 is None, find the direction of max sliced Wasserstein distance between x and gaussian\n",
    "    #if x2 is not None, find the direction of max sliced Wasserstein distance between x and x2 \n",
    "\n",
    "    if x2 != 'gaussian':\n",
    "        assert x.shape[1] == x2.shape[1]\n",
    "        if x2.shape[0] > x.shape[0]:\n",
    "            x2 = x2[torch.randperm(x2.shape[0])][:x.shape[0]]\n",
    "        elif x2.shape[0] < x.shape[0]:\n",
    "            x = x[torch.randperm(x.shape[0])][:x2.shape[0]]\n",
    "\n",
    "    ndim = x.shape[1]\n",
    "    if n_component is None:\n",
    "        n_component = ndim\n",
    "\n",
    "    q = None\n",
    "    if x2 == 'gaussian':\n",
    "        if Npercentile is None:\n",
    "            q, pg = SWD_prepare(len(x), device=x.device)\n",
    "            q = None\n",
    "        else:\n",
    "            q, pg = SWD_prepare(Npercentile, device=x.device)\n",
    "    elif Npercentile is not None:\n",
    "        q = SWD_prepare(Npercentile, device=x.device, gaussian=False)\n",
    "\n",
    "\n",
    "    #initialize w. algorithm from https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    wi = torch.randn(ndim, n_component, device=x.device)\n",
    "    Q, R = torch.qr(wi)\n",
    "    L = torch.sign(torch.diag(R))\n",
    "    w = (Q * L).T\n",
    "\n",
    "    lr = 0.1\n",
    "    down_fac = 0.5\n",
    "    up_fac = 1.5\n",
    "    c = 0.5\n",
    "\n",
    "    #algorithm from http://noodle.med.yale.edu/~hdtag/notes/steifel_notes.pdf\n",
    "    #note that here w = X.T\n",
    "    #use backtracking line search\n",
    "    w1 = w.clone()\n",
    "    w.requires_grad_(True)\n",
    "    if x2 == 'gaussian':\n",
    "        loss = -SlicedWassersteinDistanceG(w @ x.T, pg, q, p)\n",
    "    else:\n",
    "        loss = -SlicedWassersteinDistance(w @ x.T, w @ x2.T, q, p)\n",
    "    loss1 = loss\n",
    "    for i in range(maxiter):\n",
    "        GT = torch.autograd.grad(loss, w)[0]\n",
    "        w.requires_grad_(False)\n",
    "        WT = w.T @ GT - GT.T @ w\n",
    "        e = - w @ WT #dw/dlr\n",
    "        m = torch.sum(GT * e) #dloss/dlr\n",
    "\n",
    "        lr /= down_fac\n",
    "        while loss1 > loss + c*m*lr:\n",
    "            lr *= down_fac\n",
    "            if 2*n_component < ndim:\n",
    "                UT = torch.cat((GT, w), dim=0).double()\n",
    "                V = torch.cat((w.T, -GT.T), dim=1).double()\n",
    "                w1 = (w.double() - lr * w.double() @ V @ torch.pinverse(torch.eye(2*n_component, dtype=torch.double, device=x.device)+lr/2*UT@V) @ UT).to(torch.get_default_dtype())\n",
    "            else:\n",
    "                w1 = (w.double() @ (torch.eye(ndim, dtype=torch.double, device=x.device)-lr/2*WT.double()) @ torch.pinverse(torch.eye(ndim, dtype=torch.double, device=x.device)+lr/2*WT.double())).to(torch.get_default_dtype())\n",
    "\n",
    "            w1.requires_grad_(True)\n",
    "            if x2 == 'gaussian':\n",
    "                loss1 = -SlicedWassersteinDistanceG(w1 @ x.T, pg, q, p)\n",
    "            else:\n",
    "                loss1 = -SlicedWassersteinDistance(w1 @ x.T, w1 @ x2.T, q, p)\n",
    "\n",
    "        if torch.max(torch.abs(w1-w)) < eps:\n",
    "            w = w1\n",
    "            break\n",
    "\n",
    "        lr *= up_fac\n",
    "        loss = loss1\n",
    "        w = w1\n",
    "    if x2 == 'gaussian':\n",
    "        WD = SlicedWassersteinDistanceG(w @ x.T, pg, q, p, perdim=False)\n",
    "    else:\n",
    "        WD = SlicedWassersteinDistance(w @ x.T, w @ x2.T, q, p, perdim=False)\n",
    "    return w.T, WD**(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ICA_Wasserstein(x):\n",
    "    \n",
    "    A, WD = maxSWDdirection(torch.tensor(x, dtype=torch.float32))\n",
    "        \n",
    "    return A.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
